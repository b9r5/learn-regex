{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46e48210-1db5-4977-8872-d61b3c7df211",
   "metadata": {},
   "source": [
    "**Abstract.**\n",
    "I reached the point in my machine learning journey where I understood the basic concepts\n",
    "(backpropagation, regularization, CNNs, RNNs, and transformers), but wanted to dig deeper.\n",
    "The machine learning engineers that I knew suggested that I train a model for a problem I\n",
    "found interesting. I selected learning regular expressions from examples as a toy problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da333ee-3050-478c-99ad-60c0f6d3286f",
   "metadata": {},
   "source": [
    "# Generating training data\n",
    "\n",
    "My plan for generating training data was:\n",
    "\n",
    "1. Generate random regular expressions (regexes).\n",
    "2. For each regex, generate some number of strings matching the regex.\n",
    "3. For training data, let the $(x, y)$ input-output pairs be defined by:\n",
    "   * $y$, a regex\n",
    "   * $x$, a list of strings matching $y$, one per line\n",
    "4. During training, reward if a forward pass through the network, starting with input $x$,\n",
    "   produces an output $\\hat{y}$ such that $y$ and $\\hat{y}$ recognize the same strings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018fb553-7e3f-4a55-943f-577d0f662224",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1163cf48-21c2-4ec9-94db-ecc0d33acfe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import graphviz\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351284e8-c38a-4d23-b330-97f172010c2e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Automata theory conversions\n",
    "\n",
    "To work with regular expressions and finite automata, I started with\n",
    "[Megha Bose's](https://github.com/Megha-Bose)\n",
    "[Automata Theory Conversions](https://github.com/Megha-Bose/Automata-Theory-Conversions)\n",
    "code. It wasn't licensed, so I asked her permission, and she gave me her\n",
    "permission to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461baa34-a467-4c78-b9c5-d689f037cb0d",
   "metadata": {},
   "source": [
    "## Generating random regexes\n",
    "\n",
    "The following two problems did not seem like toy problems:\n",
    "\n",
    "1. Creating a machine learning system capable of inferring long regexes.\n",
    "2. Createing a machine learning system capable of inferring regexes with deep\n",
    "   nestings of union and Kleene star operations.\n",
    "\n",
    "Because I wanted a toy problem, I needed a way to generate short regexes having\n",
    "shallow nestings. I settled on constructing a random parse tree having a\n",
    "maximum depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19b03ac2-c8dd-4b14-86ab-0b0c51b3a958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Union, Kleene star, concatenation, and grouping operations.\n",
    "# '$' is special character that represents the empty string. \n",
    "# All other characters are considered symbols.\n",
    "operators = ['+', '*', '.', '(', ')']\n",
    "\n",
    "\n",
    "class OpType:\n",
    "    \"\"\"\n",
    "    Constants for regular expression operator types.\n",
    "    \"\"\"\n",
    "    SYMBOL = 1\n",
    "    CONCAT = 2\n",
    "    UNION  = 3\n",
    "    KLEENE = 4\n",
    "\n",
    "\n",
    "class ExpressionNode:\n",
    "    \"\"\"\n",
    "    Node in an expression tree that represents the parse tree for a regular\n",
    "    expression.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, op_type, value=None):\n",
    "        self.op_type = op_type\n",
    "        self.value = value\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Converts the node to a string in infix order, adding a lot of parens.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.op_type == OpType.SYMBOL:\n",
    "            return self.value\n",
    "        elif self.op_type == OpType.CONCAT:\n",
    "            left = f'({self.left})' if self.left.op_type == OpType.UNION else str(self.left)\n",
    "            right = f'({self.right})' if self.right.op_type == OpType.UNION else str(self.right)\n",
    "            return left + right\n",
    "        elif self.op_type == OpType.UNION:\n",
    "            return f'{self.left}+{self.right}'\n",
    "        elif self.op_type == OpType.KLEENE:\n",
    "            if self.left.op_type == OpType.KLEENE:\n",
    "                return str(self.left)\n",
    "            elif self.left.op_type == OpType.SYMBOL:\n",
    "                return str(self.left) + '*'\n",
    "            else:\n",
    "                return f'({self.left})*'\n",
    "        else:\n",
    "            raise Exception(f'unknown op type {self.op_type}')\n",
    "\n",
    "\n",
    "def random_regex(max_depth=5, symbols=\"abcde\", recurse_prob=0.95):\n",
    "    \"\"\"\n",
    "    Generates a random regular expression.\n",
    "\n",
    "    Args:\n",
    "      max_depth: maximum depth of the parse tree\n",
    "      symbols: allowed characters of the regular expression; must not contain \n",
    "               '$' nor any operator\n",
    "      recurse_prob: probability that this method is invoked recursively\n",
    "\n",
    "    Returns:\n",
    "      expression tree representing the regular expression\n",
    "    \"\"\"\n",
    "\n",
    "    can_recurse = max_depth > 1 and random.random() < recurse_prob\n",
    "\n",
    "    op_type = random.randint(2, 4) if can_recurse else OpType.SYMBOL\n",
    "    \n",
    "    node = ExpressionNode(op_type)\n",
    "\n",
    "    if op_type == OpType.SYMBOL:\n",
    "        node.value = random.choice(symbols)\n",
    "    else:\n",
    "        node.left = random_regex(max_depth-1, symbols, recurse_prob)\n",
    "        if op_type == OpType.CONCAT or op_type == OpType.UNION:\n",
    "            node.right = random_regex(max_depth-1, symbols, recurse_prob)\n",
    "        \n",
    "    return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93458bed-1747-4a4f-9185-69a9d707ae98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e*(bb+eb)((a+b)(c+b))*\n",
      "de+a+e+(e+b)*+(b+c+d)(b+c+a*)\n",
      "(e*+a+b)(be+b+d)(e*+e*)*\n",
      "(bbb(e+a))*\n",
      "e*\n",
      "(c*(a+c)(c*+b))*\n",
      "((cb)*+c*)(ea+a+e)b*d*\n",
      "e\n",
      "((e+d)*+eeaa)*\n",
      "(d+b)e*(b+a)(c+d)a*\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(random_regex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167e046e-9e9c-44fb-93ba-e8cc9ca5a1db",
   "metadata": {},
   "source": [
    "We've now generated random regexes, so we turn to generating strings matching a single regex.\n",
    "\n",
    "## Generating strings matching a regex\n",
    "\n",
    "To generate strings matching a regex, I first converted the regex into a non-deterministic\n",
    "finite automaton (NFA), then followed random paths through the NFA from an initial node to\n",
    "a terminal node.\n",
    "\n",
    "### Step 1: rewrite regex (add concatenation, convert to postfix)\n",
    "\n",
    "The first step in computing a regex to an NFA is to parse it. For this, it's useful to\n",
    "rewrite the regex in a couple of ways:\n",
    "\n",
    "1. Add concatenation symbols for convenience.\n",
    "2. Convert the regex to postfix order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9808c69c-0be5-4c58-829d-fe21d636bdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_concat(regex):\n",
    "    \"\"\"\n",
    "    Adds concatenation operators ('.') to the input regex for convenience.\n",
    "    \n",
    "    Args:\n",
    "      regex: a string representing the regular expression\n",
    "      \n",
    "    Returns:\n",
    "      a list representing the regular expression with added concatenation operators\n",
    "    \"\"\"\n",
    "    \n",
    "    global non_symbols\n",
    "\n",
    "    result = []\n",
    "    for i in range(len(regex) - 1):\n",
    "        result.append(regex[i])\n",
    "        if regex[i] not in operators:\n",
    "            if regex[i + 1] not in operators or regex[i + 1] == '(':\n",
    "                result += '.'\n",
    "        elif regex[i] == ')' and regex[i + 1] == '(':\n",
    "            result += '.'\n",
    "        elif regex[i] == '*' and regex[i + 1] == '(':\n",
    "            result += '.'\n",
    "        elif regex[i] == '*' and regex[i + 1] not in operators:\n",
    "            result += '.'\n",
    "        elif regex[i] == ')' and regex[i + 1] not in operators:\n",
    "            result += '.'\n",
    "\n",
    "    result += regex[-1]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30138fea-f04c-4f29-ab4b-71b5e2ab7557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', '.', 'b', '*']\n",
      "['(', 'a', '.', 'b', ')', '*']\n",
      "['(', 'a', '+', 'b', ')', '*', '.', 'c']\n"
     ]
    }
   ],
   "source": [
    "print(add_concat(\"ab*\"))\n",
    "print(add_concat(\"(ab)*\"))\n",
    "print(add_concat(\"(a+b)*c\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0112db8a-8d38-4582-992a-8ebf17414237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_precedence(a, b):\n",
    "    \"\"\"\n",
    "    Compares the precendence of the +, ., and * operators.\n",
    "    \n",
    "    Args:\n",
    "      a: the first operator\n",
    "      b: the second operator\n",
    "      \n",
    "    Returns:\n",
    "      True iff a has precedence over b\n",
    "    \"\"\"\n",
    "    p = [\"+\", \".\", \"*\"]\n",
    "    return p.index(a) > p.index(b)\n",
    "\n",
    "\n",
    "def compute_postfix(regexp):\n",
    "    \"\"\"\n",
    "    Converts an input regex to postfix order.\n",
    "    \n",
    "    Args:\n",
    "      regex: list of regex operators and symbols\n",
    "      \n",
    "    Returns:\n",
    "      list representing the regular expression in postfix order\n",
    "    \"\"\"\n",
    "    \n",
    "    stack = []\n",
    "    result = []\n",
    "\n",
    "    for c in regexp:\n",
    "        if c not in operators or c == \"*\":\n",
    "            result += c\n",
    "        elif c == \")\":\n",
    "            while len(stack) > 0 and stack[-1] != \"(\":\n",
    "                result += stack.pop()\n",
    "            stack.pop()\n",
    "        elif c == \"(\":\n",
    "            stack.append(c)\n",
    "        elif len(stack) == 0 or stack[-1] == \"(\" or compare_precedence(c, stack[-1]):\n",
    "            stack.append(c)\n",
    "        else:\n",
    "            while len(stack) > 0 and stack[-1] != \"(\" and not compare_precedence(c, stack[-1]):\n",
    "                result += stack.pop()\n",
    "            stack.append(c)\n",
    "\n",
    "    while len(stack) > 0:\n",
    "        result += stack.pop()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa53c8e8-dc4f-4012-8911-60bbe29a83e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'b', '.']\n",
      "['a', 'b', '.']\n",
      "['a', 'b', '*', '.']\n",
      "['a', 'b', '.', '*']\n",
      "['a', 'b', '+', '*', 'c', '.']\n"
     ]
    }
   ],
   "source": [
    "print(compute_postfix(['a', '.', 'b']))\n",
    "print(compute_postfix(['(', 'a', '.', 'b', ')']))\n",
    "print(compute_postfix(['a', '.', 'b', '*']))\n",
    "print(compute_postfix(['(', 'a', '.', 'b', ')', '*']))\n",
    "print(compute_postfix(['(', 'a', '+', 'b', ')', '*', '.', 'c']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56116bc1-6fff-4c3b-bcb8-e9c8dfc06a3c",
   "metadata": {},
   "source": [
    "### Step 2: create expression tree from regex\n",
    "\n",
    "Having converted the regex to postfix order, we create an expression tree from it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3624189-0cd8-4328-a3c3-531e44d334a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_expression_tree(regexp):\n",
    "    \"\"\"\n",
    "    Makes an expression tree from a regex.\n",
    "    \n",
    "    Args:\n",
    "      regex: a regular expression in postfix form\n",
    "      \n",
    "    Returns:\n",
    "      an expression tree\n",
    "    \"\"\"\n",
    "    \n",
    "    stack = []\n",
    "\n",
    "    for c in regexp:\n",
    "        if c == \"+\":\n",
    "            z = ExpressionNode(OpType.UNION)\n",
    "            z.right = stack.pop()\n",
    "            z.left = stack.pop()\n",
    "            stack.append(z)\n",
    "        elif c == \".\":\n",
    "            z = ExpressionNode(OpType.CONCAT)\n",
    "            z.right = stack.pop()\n",
    "            z.left = stack.pop()\n",
    "            stack.append(z)\n",
    "        elif c == \"*\":\n",
    "            z = ExpressionNode(OpType.KLEENE)\n",
    "            z.left = stack.pop() \n",
    "            stack.append(z)\n",
    "        elif c == \"(\" or c == \")\":\n",
    "            continue  # just for safety; our input does not contain parens\n",
    "        else:\n",
    "            stack.append(ExpressionNode(OpType.SYMBOL, c))\n",
    "    return stack[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d82de9e-8028-433b-944a-cbee0446a7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ab\n",
      "ab*\n",
      "(ab)*\n",
      "(a+b)*c\n"
     ]
    }
   ],
   "source": [
    "print(make_expression_tree(['a', 'b', '.']))\n",
    "print(make_expression_tree(['a', 'b', '*', '.']))\n",
    "print(make_expression_tree(['a', 'b', '.', '*']))\n",
    "print(make_expression_tree(['a', 'b', '+', '*', 'c', '.']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8f2bbc-a626-4f65-9215-20662287258c",
   "metadata": {},
   "source": [
    "### Step 3: convert expression tree into a NFA\n",
    "\n",
    "Having parsed the regex into a tree, we now convert the tree into a nondeterministic finite automaton (NFA).\n",
    "\n",
    "We use `$` to denote the empty string; in textbooks $\\epsilon$ normally denotes the empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54818dcf-c653-421e-affe-459ec12fa6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NFAState:\n",
    "    def __init__(self):\n",
    "        self.next_state = {}\n",
    "\n",
    "    def __str__(self):\n",
    "        lines = ['NFAState ' + str(id(self))]\n",
    "        for symbol, states in self.next_state.items():\n",
    "            for state in states: \n",
    "                lines.append(f'\\t{symbol}:\\t{str(id(state))}')\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "def compute_nfa(node):\n",
    "    \"\"\"\n",
    "    Compute the NFA for an ExpressionNode.\n",
    "    \n",
    "    Args:\n",
    "      node: an ExpressionNode\n",
    "      \n",
    "    Returns:\n",
    "      the start and end NFAState for node\n",
    "    \"\"\"\n",
    "    \n",
    "    if node.op_type == OpType.CONCAT:\n",
    "        left_nfa  = compute_nfa(node.left)\n",
    "        right_nfa = compute_nfa(node.right)\n",
    "\n",
    "        # '$' represents the empty string (usually denoted epsilon)\n",
    "        left_nfa[1].next_state['$'] = [right_nfa[0]]\n",
    "        return left_nfa[0], right_nfa[1]\n",
    "\n",
    "    elif node.op_type == OpType.UNION:\n",
    "        start = NFAState()\n",
    "        end = NFAState()\n",
    "\n",
    "        first_nfa = compute_nfa(node.left)\n",
    "        second_nfa = compute_nfa(node.right)\n",
    "\n",
    "        start.next_state['$'] = [first_nfa[0], second_nfa[0]]\n",
    "        first_nfa[1].next_state['$'] = [end]\n",
    "        second_nfa[1].next_state['$'] = [end]\n",
    "\n",
    "        return start, end\n",
    "\n",
    "    elif node.op_type == OpType.KLEENE:\n",
    "        start = NFAState()\n",
    "        end = NFAState()\n",
    "\n",
    "        starred_nfa = compute_nfa(node.left)\n",
    "\n",
    "        start.next_state['$'] = [starred_nfa[0], end]\n",
    "        starred_nfa[1].next_state['$'] = [starred_nfa[0], end]\n",
    "        \n",
    "        return start, end\n",
    "\n",
    "    elif node.op_type == OpType.SYMBOL:\n",
    "        start = NFAState()\n",
    "        end = NFAState()\n",
    "        start.next_state[node.value] = [end]\n",
    "        return start, end\n",
    "    \n",
    "    else:\n",
    "        raise Exception(f'unknown op type {node.op_type}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da31743a-2329-4555-afe3-dd1a3953cee9",
   "metadata": {},
   "source": [
    "### Putting steps 1-3 together\n",
    "\n",
    "We now wrap up steps 1-3 in a function that takes a regex as argument and returns an NFA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "22164b18-2640-4e4e-bb79-2028db267d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_to_nfa(regex):\n",
    "    \"\"\"\n",
    "    Compute the NFA for a regular expression.\n",
    "    \n",
    "    Args:\n",
    "      regex: a regular expression\n",
    "    \n",
    "    Returns:\n",
    "      the start and end NFAState\n",
    "    \"\"\"\n",
    "    \n",
    "    concat = add_concat(regex)\n",
    "    postfix = compute_postfix(concat)\n",
    "    tree = make_expression_tree(postfix)\n",
    "    start, end = compute_nfa(tree)\n",
    "    return start, end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47da7913-bbfb-412f-9245-5bee87dbef29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<__main__.NFAState object at 0x7fda2d6b9de0>, <__main__.NFAState object at 0x7fda2d6b9a50>)\n",
      "(<__main__.NFAState object at 0x7fda2d6b9e10>, <__main__.NFAState object at 0x7fda2d6ba860>)\n",
      "(<__main__.NFAState object at 0x7fda2d6b94b0>, <__main__.NFAState object at 0x7fda2d6b9ab0>)\n",
      "(<__main__.NFAState object at 0x7fda2d6ba2f0>, <__main__.NFAState object at 0x7fda2d6bbc10>)\n"
     ]
    }
   ],
   "source": [
    "print(regex_to_nfa(\"ab*\"))\n",
    "print(regex_to_nfa(\"(ab)*\"))\n",
    "print(regex_to_nfa(\"(a+b)*c\"))\n",
    "print(regex_to_nfa(str(random_regex(6))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41cb82f-69c3-4e74-9f7f-c36cbd7e4164",
   "metadata": {},
   "source": [
    "### Step 4: random paths through an NFA\n",
    "\n",
    "Starting with an NFA, we've generated a regex. The next step is to generate strings matching\n",
    "the regex from the NFA. To do that, we follow a random path through the NFA from the initial\n",
    "node. We build the sequence of edge labels we see along the path traveled so far. When we reach\n",
    "the terminal node, we output the edge labels we saw."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c306a13c-f61f-4ace-9680-1036956ef8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nfa_path(start, end, max_steps):\n",
    "    \"\"\"\n",
    "    Follow a random path through an NFA, accumulating edge labels.\n",
    "    \n",
    "    Args:\n",
    "      start: start NFAState\n",
    "      end: end NFAState\n",
    "      \n",
    "    returns:\n",
    "      ':' followed by edge labels, or an empty string if no path\n",
    "      to end state of at most max_steps steps was found\n",
    "    \"\"\"\n",
    "    \n",
    "    step = 0\n",
    "    state = start\n",
    "    found_path = False\n",
    "    labels = []\n",
    "    \n",
    "    while step < max_steps:\n",
    "        if state == end:\n",
    "            found_path = True\n",
    "            break\n",
    "            \n",
    "        # select random label\n",
    "        i = random.randint(0, len(state.next_state) - 1)\n",
    "        label = list(state.next_state)[i]\n",
    "        if label != '$':\n",
    "            labels.append(label)\n",
    "        \n",
    "        # select random next state\n",
    "        i = random.randint(0, len(state.next_state[label]) - 1)\n",
    "        state = state.next_state[label][i]\n",
    "    \n",
    "    if found_path:\n",
    "        return ':' + ''.join(labels)\n",
    "    else:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b81835b0-45f5-4607-9903-5326c1631bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "regex ab*\n",
      ":abbbbb\n",
      ":abbb\n",
      ":a\n",
      ":a\n",
      ":ab\n",
      ":ab\n",
      ":abb\n",
      ":abbbb\n",
      ":a\n",
      ":a\n",
      "-----\n",
      "regex (ab)*\n",
      ":\n",
      ":\n",
      ":\n",
      ":ab\n",
      ":\n",
      ":\n",
      ":\n",
      ":\n",
      ":ab\n",
      ":\n",
      "-----\n",
      "regex (a+b)*c*\n",
      ":\n",
      ":\n",
      ":\n",
      ":\n",
      ":acc\n",
      ":acc\n",
      ":abc\n",
      ":b\n",
      ":c\n",
      ":\n",
      "-----\n",
      "regex ba+b+c*+c\n",
      ":\n",
      ":ccc\n",
      ":c\n",
      ":b\n",
      ":c\n",
      ":c\n",
      ":c\n",
      ":c\n",
      ":c\n",
      ":c\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "regexes = ['ab*', '(ab)*', '(a+b)*c*', str(random_regex(4, 'abc'))]\n",
    "\n",
    "for regex in regexes:\n",
    "    print('regex', regex)\n",
    "    start, end = regex_to_nfa(regex)\n",
    "    for i in range(0, 10):\n",
    "        print(nfa_path(start, end, 8))\n",
    "    print('-----')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2417d60-a1ae-4395-a055-8f4d639d80c6",
   "metadata": {},
   "source": [
    "### Step 5: create training data\n",
    "\n",
    "We now generate training data as files in a data directory. I decided to start\n",
    "simple, with regexes having a maximum depth of 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f252ce8a-234f-4ac0-981d-adb720eeba4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(files, examples_per_file,\n",
    "                         max_depth=4, symbols=\"abcdefghijklmnop\",\n",
    "                         recurse_prob=0.95):\n",
    "    \"\"\"\n",
    "    Generate training data. Each file contains a regex and some strings\n",
    "    matching the regex.\n",
    "\n",
    "    Args:\n",
    "      files: number of files to create\n",
    "      examples_per_file: number of matching strings per file\n",
    "      max_depth: see random_regex()\n",
    "      symbols: see random_regex()\n",
    "      recurse_prob: see random_regex()\n",
    "    \"\"\"\n",
    "    \n",
    "    os.mkdir('data')\n",
    "    \n",
    "    for i in range(files):\n",
    "        with open(f'data/{i:06}.regex', 'x') as file:\n",
    "            regex = str(random_regex(max_depth=max_depth, symbols=symbols,\n",
    "                                     recurse_prob=recurse_prob))\n",
    "\n",
    "            start, end = regex_to_nfa(regex)\n",
    "\n",
    "            # choose a reasonable number of steps through the NFA.\n",
    "            # empirically allowing max_steps = 4 * examples is enough\n",
    "            # to frequently reach an accepting state\n",
    "            max_steps = examples_per_file * 4\n",
    "            \n",
    "            for i in range(examples_per_file):\n",
    "                path = ''\n",
    "                # repeat until we find a path to an accepting state:\n",
    "                while not path.startswith(':'):\n",
    "                    path = nfa_path(start, end, max_steps)\n",
    "\n",
    "                file.write(f'{path}\\n')\n",
    "\n",
    "            # indicate that no more examples follow with the prompt '>'\n",
    "            file.write('>\\n')\n",
    "\n",
    "            # finally, write the regex\n",
    "            file.write(f'{regex}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1746ae9-0df8-4e97-b81c-41af1f57bb93",
   "metadata": {},
   "source": [
    "I decided to generate one million regexes worth of training data. Empirically,\n",
    "I found that when using the function `nfa_path()`, there were at most 900\n",
    "unique strings generated per regex. It seemed reasonable to generate some\n",
    "repeats, both to allow training to guess the structure of the regex, and to\n",
    "allow `nfa_path()` to cover the NFA's structure. So I chose to generate 10,000\n",
    "examples per file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a524a5fb-11b4-4166-95f0-c84d9753817b",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = 1_000           # replace with: 1_000_000\n",
    "examples_per_file = 10  # replace with: 10_000\n",
    "create_training_data(files, examples_per_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f109782-d655-4737-8303-122539c42181",
   "metadata": {},
   "source": [
    "And there we have it -- one million training files!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b234492b-1c1e-48be-b102-0fd76a926139",
   "metadata": {},
   "source": [
    "## Determining if two regexes are equivalent\n",
    "\n",
    "We will need to define our loss function such that it rewards a prediction\n",
    "$\\hat{y}$ if $y$ and $\\hat{y}$ recognize the same strings. How do we tell\n",
    "whether two regexes recognize the same strings? The classic algorithm\n",
    "has us do the following:\n",
    "\n",
    "1. Convert $y$ and $\\hat{y}$ to NFAs $N$ and $\\hat{N}$.\n",
    "2. Convert $N$ and $\\hat{N}$ to deterministic finite automata (DFAs) $D$ and\n",
    "   $\\hat{D}$.\n",
    "3. Use the table-filling algorithm to determine if the start states of $D$ and\n",
    "   $\\hat{D}$ are equivalent.\n",
    "\n",
    "In this section, we develop the code necessary for steps 2 and 3. Step 1 was\n",
    "already covered when we converted a regex to an NFA for the purpose of\n",
    "generating strings matching the regex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3dc3e3-dc0e-4425-b067-c63da5784e21",
   "metadata": {},
   "source": [
    "### Step 1: convert regex to NFA\n",
    "\n",
    "Automata-Theory-Constructions already had code for this, which I modified a little (see `regex_to_nfa()`)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7e7c4f-053f-45b1-a6e6-ae26146545eb",
   "metadata": {},
   "source": [
    "### Step 2: Convert NFA to DFA\n",
    "\n",
    "To convert a nondeterministic finite automaton to a deterministic finite\n",
    "automaton that recognizes the same language, we use the classic subset\n",
    "construction. (A good description of the subset construction can be found in\n",
    "M. Sipser, _Introduction to the Theory of Computation_).\n",
    "\n",
    "We first write a function to compute the set of all NFA states reachable from\n",
    "a given NFA state by following only transitions labeled with $\\epsilon$, the\n",
    "empty string. Recall that $\\epsilon$ is represented by `$` in our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d6b8ce0-76c9-4810-94a5-dda9ff023c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_closure(states):\n",
    "    \"\"\"\n",
    "    Compute the set of all states that can be reached from the given states by\n",
    "    following only epsilon ($) transitions. \n",
    "\n",
    "    Args:\n",
    "      states: iterable of NFAState\n",
    "      visited: set of already visited states\n",
    "\n",
    "    Returns:\n",
    "      frozenset of states reachable from states by following only epsilon ($)\n",
    "      transitions.\n",
    "    \"\"\"\n",
    "    \n",
    "    def helper(states, visited):    \n",
    "        for state in states:\n",
    "            if state not in visited:\n",
    "                visited.add(state)\n",
    "                try:\n",
    "                    helper(state.next_state['$'], visited)\n",
    "                except KeyError:\n",
    "                    pass\n",
    "\n",
    "    visited = set()\n",
    "    helper(states, visited)\n",
    "    \n",
    "    return frozenset(visited)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f89574-0ab7-4d08-a94f-0bd0384647cf",
   "metadata": {},
   "source": [
    "To test `epsilon_closure()`, we use a couple of NFAs with $\\epsilon$ transitions. The first is structured like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4157a7a7-9416-421d-a728-e2314039abc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"134pt\" height=\"305pt\"\n",
       " viewBox=\"0.00 0.00 134.00 305.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 301)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-301 130,-301 130,4 -4,4\"/>\n",
       "<!-- a -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>a</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"62\" cy=\"-279\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"62\" y=\"-275.3\" font-family=\"Times,serif\" font-size=\"14.00\">a</text>\n",
       "</g>\n",
       "<!-- b -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>b</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-192\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">b</text>\n",
       "</g>\n",
       "<!-- a&#45;&gt;b -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>a&#45;&gt;b</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M55.09,-261.21C50.12,-249.14 43.32,-232.64 37.66,-218.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"40.83,-217.38 33.78,-209.47 34.35,-220.05 40.83,-217.38\"/>\n",
       "<text text-anchor=\"middle\" x=\"50.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">ϵ</text>\n",
       "</g>\n",
       "<!-- c -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>c</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"99\" cy=\"-192\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"99\" y=\"-188.3\" font-family=\"Times,serif\" font-size=\"14.00\">c</text>\n",
       "</g>\n",
       "<!-- a&#45;&gt;c -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>a&#45;&gt;c</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M69.13,-261.61C74.37,-249.58 81.6,-232.98 87.63,-219.13\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"90.98,-220.19 91.76,-209.63 84.56,-217.4 90.98,-220.19\"/>\n",
       "<text text-anchor=\"middle\" x=\"85.5\" y=\"-231.8\" font-family=\"Times,serif\" font-size=\"14.00\">ϵ</text>\n",
       "</g>\n",
       "<!-- d -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>d</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"63\" cy=\"-105\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"63\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\">d</text>\n",
       "</g>\n",
       "<!-- b&#45;&gt;d -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>b&#45;&gt;d</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M34.11,-174.21C39.22,-162.14 46.21,-145.64 52.03,-131.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"55.35,-133.04 56.02,-122.47 48.9,-130.31 55.35,-133.04\"/>\n",
       "<text text-anchor=\"middle\" x=\"51.5\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">ϵ</text>\n",
       "</g>\n",
       "<!-- c&#45;&gt;d -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>c&#45;&gt;d</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M91.89,-174.21C86.78,-162.14 79.79,-145.64 73.97,-131.89\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"77.1,-130.31 69.98,-122.47 70.65,-133.04 77.1,-130.31\"/>\n",
       "<text text-anchor=\"middle\" x=\"86.5\" y=\"-144.8\" font-family=\"Times,serif\" font-size=\"14.00\">ϵ</text>\n",
       "</g>\n",
       "<!-- e -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>e</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"63\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"63\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">e</text>\n",
       "</g>\n",
       "<!-- d&#45;&gt;e -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>d&#45;&gt;e</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M63,-86.8C63,-75.16 63,-59.55 63,-46.24\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"66.5,-46.18 63,-36.18 59.5,-46.18 66.5,-46.18\"/>\n",
       "<text text-anchor=\"middle\" x=\"66.5\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\">x</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fda2d6ba3b0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot = graphviz.Digraph()\n",
    "dot.node('a')\n",
    "dot.node('b')\n",
    "dot.node('c')\n",
    "dot.node('d')\n",
    "dot.node('e')\n",
    "dot.edge('a', 'b', 'ϵ')\n",
    "dot.edge('a', 'c', 'ϵ')\n",
    "dot.edge('b', 'd', 'ϵ')\n",
    "dot.edge('c', 'd', 'ϵ')\n",
    "dot.edge('d', 'e', 'x')\n",
    "dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5559344-d0a2-4c2a-86ea-9f69184b3b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = NFAState()\n",
    "b = NFAState()\n",
    "c = NFAState()\n",
    "d = NFAState()\n",
    "e = NFAState()\n",
    "\n",
    "a.next_state['$'] = [b, c]\n",
    "b.next_state['$'] = [d]\n",
    "c.next_state['$'] = [d]\n",
    "d.next_state['x'] = [e]\n",
    "\n",
    "assert epsilon_closure([a]) == {a, b, c, d}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b033463f-b3cd-4ca0-ac9f-da3c3f6b82ba",
   "metadata": {},
   "source": [
    "The second graph is structured as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0817a8ce-3f74-4a08-9d73-f61b15ade31e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.43.0 (0)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"87pt\" height=\"44pt\"\n",
       " viewBox=\"0.00 0.00 87.00 44.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 40)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-40 83,-40 83,4 -4,4\"/>\n",
       "<!-- f -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>f</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">f</text>\n",
       "</g>\n",
       "<!-- f&#45;&gt;f -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>f&#45;&gt;f</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.44,-24.69C63.03,-25.15 72,-22.92 72,-18 72,-14.77 68.14,-12.7 62.49,-11.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"62.6,-8.29 52.44,-11.31 62.27,-15.28 62.6,-8.29\"/>\n",
       "<text text-anchor=\"middle\" x=\"75.5\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\">ϵ</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x7fda2d6bb4c0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot = graphviz.Digraph()\n",
    "dot.edge('f', 'f', 'ϵ')\n",
    "dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eda47dcf-579c-4d1d-bae1-1b14529d4c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = NFAState()\n",
    "f.next_state['$'] = [f]\n",
    "\n",
    "assert epsilon_closure([f]) == {f}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed122892-24a5-40ef-99d5-2dd3a620f1c1",
   "metadata": {},
   "source": [
    "Having written a function to compute the $\\epsilon$-closure of a collection of\n",
    "states, we now write a function that converts an NFA to a DFA. This is the\n",
    "well-known subset construction. We find the reachable sets of states using\n",
    "depth first search. Given a set of NFA states constituting a DFA state, and a\n",
    "symbol, the search finds the set of NFA states reachable from the DFA state by\n",
    "following the symbol.\n",
    "\n",
    "Unlike many presentations of the subset construction, we don't compute the\n",
    "power set of NFA states up front, and instead let search find the reachable DFA\n",
    "states for us. We might avoid creating all $2^{|Q|}$ subsets of the NFA states\n",
    "$Q$ in some cases (though in the worst case we will visit $\\Omega(2^{|Q|})$\n",
    "states). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4f2e21e-f414-4876-bf92-3cf95bb0d462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A state in a deterministic finite automaton.\n",
    "class DFAState:\n",
    "    def __init__(self, is_accepting):\n",
    "        self.is_accepting = is_accepting\n",
    "        self.next_state = {}\n",
    "\n",
    "    def __str__(self):\n",
    "        lines = ['DFAState ' + str(id(self))]\n",
    "        if self.is_accepting:\n",
    "            lines.append('accepting')\n",
    "        for symbol, state in self.next_state.items():\n",
    "            lines.append(f'\\t{symbol}:\\t{str(id(state))}')\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "# A deterministic finite automaton.\n",
    "class DFA:\n",
    "    def __init__(self, states, start, symbols):\n",
    "        self.states = states\n",
    "        self.start = start\n",
    "        self.symbols = symbols\n",
    "\n",
    "    def __str__(self):\n",
    "        lines = []\n",
    "        for state in self.states:\n",
    "            lines.append('-----')\n",
    "            lines.append(str(id(state)))\n",
    "            if state == self.start:\n",
    "                lines.append('start')\n",
    "            if state.is_accepting:\n",
    "                lines.append('accepting')\n",
    "            for symbol, state in state.next_state.items():\n",
    "                lines.append(f'\\t{symbol}:\\t{str(id(state))}')\n",
    "        return '\\n'.join(lines)\n",
    "\n",
    "\n",
    "def nfa_to_dfa(start, end, symbols):\n",
    "    \"\"\"\n",
    "    Convert an NFA to a DFA.\n",
    "\n",
    "    Args:\n",
    "      start: the start NFAState\n",
    "      end: the end NFAState\n",
    "      symbols: the alphabet of the NFA\n",
    "\n",
    "    Returns:\n",
    "      a DFA\n",
    "    \"\"\"\n",
    "\n",
    "    def is_accepting(nfa_states):\n",
    "        return any(s == end for s in nfa_states)\n",
    "    \n",
    "    dfa_states = {} # key: set of NFAState, value: DFAState\n",
    "    nfa_states = {} # key: DFAState, value: set of NFAState\n",
    "\n",
    "    closure = epsilon_closure([start])\n",
    "    start_state = DFAState(is_accepting(closure))\n",
    "    dfa_states[closure] = start_state\n",
    "    nfa_states[start_state] = closure\n",
    "\n",
    "    frontier = [start_state]\n",
    "    visited = set()\n",
    "    \n",
    "    while len(frontier) > 0:\n",
    "        state = frontier.pop()\n",
    "\n",
    "        if state in visited:\n",
    "            continue\n",
    "\n",
    "        for symbol in symbols:\n",
    "            # find NFA states reachable from state by following symbol\n",
    "            reachable = set()\n",
    "\n",
    "            for nfa_state in nfa_states[state]:\n",
    "                try:\n",
    "                    reachable.update(nfa_state.next_state[symbol])\n",
    "                except KeyError:\n",
    "                    pass\n",
    "\n",
    "            reachable = epsilon_closure(reachable)\n",
    "            \n",
    "            if reachable in dfa_states:\n",
    "                next = dfa_states[reachable]\n",
    "            else:\n",
    "                next = DFAState(is_accepting(reachable))\n",
    "                dfa_states[reachable] = next\n",
    "                nfa_states[next] = reachable\n",
    "            \n",
    "            state.next_state[symbol] = next\n",
    "            \n",
    "            if next not in visited:\n",
    "                frontier.append(next)\n",
    "        \n",
    "        visited.add(state)\n",
    "    \n",
    "    return DFA(visited, start_state, symbols) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4c33c1e-f348-4676-8402-0df05629d93d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFA for a:\n",
      "-----\n",
      "140575041566080\n",
      "accepting\n",
      "\ta:\t140575041558544\n",
      "-----\n",
      "140575041558544\n",
      "\ta:\t140575041558544\n",
      "-----\n",
      "140575041565504\n",
      "start\n",
      "\ta:\t140575041566080\n",
      "DFA for b*:\n",
      "-----\n",
      "140575041559216\n",
      "accepting\n",
      "\tb:\t140575041559216\n",
      "-----\n",
      "140575041564016\n",
      "start\n",
      "accepting\n",
      "\tb:\t140575041559216\n"
     ]
    }
   ],
   "source": [
    "regexes = ['a', 'b*']\n",
    "symbols = ['a', 'b']\n",
    "\n",
    "for i in range(len(regexes)):\n",
    "    start, end = regex_to_nfa(regexes[i])\n",
    "    dfa = nfa_to_dfa(start, end, symbols[i])\n",
    "    print(f'DFA for {regexes[i]}:')\n",
    "    print(str(dfa))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8af9350-0c19-4cd1-bb4f-e2fef9b59e59",
   "metadata": {},
   "source": [
    "### Step 3: determine if two DFAs recognize the same strings\n",
    "\n",
    "We first introduce a disjoint sets data structure based on the union-find algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a7e67f0-edc1-48a7-b021-e26163616436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A disjoint sets data structure.\n",
    "class UnionFind:\n",
    "    def __init__(self, items):\n",
    "        self.parent = {i: i for i in items}\n",
    "        self.size = {i: 1 for i in items}\n",
    "\n",
    "    def find(self, item):\n",
    "        \"\"\"\n",
    "        Find the representative of an item.\n",
    "\n",
    "        Args:\n",
    "          item: the item for which the representative is wanted\n",
    "\n",
    "        Returns:\n",
    "          the representative of item\n",
    "        \"\"\"\n",
    "        if self.parent[item] == item:\n",
    "            return item\n",
    "        else:\n",
    "            parent = self.find(self.parent[item])\n",
    "            self.parent[item] = parent\n",
    "            return parent\n",
    "\n",
    "    def union(self, item0, item1):\n",
    "        \"\"\"\n",
    "        Merge two items.\n",
    "\n",
    "        Args:\n",
    "          item0 an item\n",
    "          item1 another item (but could also be the same as item0)\n",
    "        \"\"\"\n",
    "        find0 = self.find(item0)\n",
    "        find1 = self.find(item1)\n",
    "        if find0 == find1:\n",
    "            return\n",
    "        if self.size[find0] < self.size[find1]:\n",
    "            find0, find1 = find1, find0\n",
    "        self.parent[find1] = find0\n",
    "        self.size[find0] += self.size[find1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "906ab4a3-55fc-4135-acd2-24d01fd93871",
   "metadata": {},
   "outputs": [],
   "source": [
    "uf = UnionFind(range(6))\n",
    "\n",
    "for i in range(6):\n",
    "    for j in range(i):\n",
    "        assert uf.find(i) != uf.find(j)\n",
    "\n",
    "uf.union(0, 3)\n",
    "uf.union(1, 4)\n",
    "uf.union(2, 5)\n",
    "\n",
    "for i in range(6):\n",
    "    for j in range(6):\n",
    "        assert (uf.find(i) == uf.find(j)) == (i % 3 == j % 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e1505b-5c54-45e9-b70f-a80501ec1dd1",
   "metadata": {},
   "source": [
    "Next we write a function that determines if two DFA states $q_0$ and $q_1$ are\n",
    "equivalent, in the sense that for all strings $s$, the state resulting from\n",
    "following $s$ starting at $q_0$ is final if and only if the state resulting\n",
    "from following $s$ starting at $q_1$ is final.\n",
    "\n",
    "The algorithm for this is known as the \"table-filling\" algorithm. A good\n",
    "description can be found in\n",
    "[these lecture notes](https://people.csail.mit.edu/rrw/6.045-2020/lec5-color.pdf)\n",
    "by Ryan Williams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5e158066-2d31-4ba5-8a5f-de29cf9a57dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equivalent_states(dfa):\n",
    "    \"\"\"\n",
    "    Compute the equivalent states of a DFA.\n",
    "\n",
    "    Args:\n",
    "      dfa: a DFA\n",
    "\n",
    "    Returns:\n",
    "      a UnionFind such that find(state0) == find(state1) iff state0 and state1\n",
    "      are equivalent\n",
    "    \"\"\"\n",
    "\n",
    "    different = set()\n",
    "\n",
    "    for s0 in dfa.states:\n",
    "        for s1 in dfa.states:\n",
    "            if s0 == s1:\n",
    "                continue\n",
    "            if s0.is_accepting ^ s1.is_accepting:\n",
    "                different.add((s0, s1))\n",
    "                different.add((s1, s0))\n",
    "\n",
    "    stop = False\n",
    "\n",
    "    while not stop:\n",
    "        stop = True\n",
    "        \n",
    "        for s0 in dfa.states:\n",
    "            for s1 in dfa.states:\n",
    "                if s0 == s1 or (s0, s1) in different:\n",
    "                    continue\n",
    "\n",
    "                for symbol in dfa.symbols:\n",
    "                    next0 = s0.next_state[symbol]\n",
    "                    next1 = s1.next_state[symbol]\n",
    "                    if (next0, next1) in different:\n",
    "                        different.add((s0, s1))\n",
    "                        different.add((s1, s0))\n",
    "                        stop = False\n",
    "\n",
    "    result = UnionFind(dfa.states)\n",
    "    for s0 in dfa.states:\n",
    "        for s1 in dfa.states:\n",
    "            if (s0, s1) not in different:\n",
    "                result.union(s0, s1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f682b40-e1b7-4b38-916c-eb1ed270bc94",
   "metadata": {},
   "source": [
    "We can use use our function that computes the equivalent states of a DFA to\n",
    "also determine whether two DFAs are equivalent. The idea is to form a \"union\"\n",
    "DFA $D = (Q_0 \\cup Q_1, \\Sigma, \\delta_0 \\cup \\delta_1, -, F_0 \\cup F_1)$,\n",
    "then test whether $q_0$ and $q_1$ are equivalent in $D$. The start state\n",
    "doesn't matter, hence the \"$-$\". The idea comes from\n",
    "[these lecture notes by Ana Bove](https://www.cse.chalmers.se/edu/year/2012/course/TMV026/lec10.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6d5f213b-c170-4c93-803d-fc01b9a2d16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_equivalent_dfas(dfa0, dfa1):\n",
    "    \"\"\"\n",
    "    Determine whether two DFAs have the same language.\n",
    "\n",
    "    Args:\n",
    "      dfa0: a DFA\n",
    "      dfa1: another DFA, which must have states disjoint from dfa0\n",
    "\n",
    "    Returns:\n",
    "      True iff dfa0 and dfa1 have the same language\n",
    "    \"\"\"\n",
    "\n",
    "    if sorted(dfa0.symbols) != sorted(dfa1.symbols):\n",
    "        return False\n",
    "    \n",
    "    states = set(dfa0.states)\n",
    "    states.update(dfa1.states)\n",
    "\n",
    "    # initial state does not matter (it could be anything)\n",
    "    union = DFA(states, dfa0.start, dfa0.symbols)\n",
    "\n",
    "    equiv = equivalent_states(union)\n",
    "\n",
    "    return equiv.find(dfa0.start) == equiv.find(dfa1.start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f5e9f9-3de7-476c-9b42-08162540a167",
   "metadata": {},
   "source": [
    "### Wrapping up steps 1-3\n",
    "\n",
    "We finish up with a function to test the equivalence of two regular\n",
    "expressions. First we write a function that extracts the symbols from a regular\n",
    "expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "42a204c5-faa1-416c-ac1b-20371fb21089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regex_to_symbols(regex):\n",
    "    \"\"\"\n",
    "    Given a regular expression, return its symbols.\n",
    "\n",
    "    Args:\n",
    "      regex: a regular expression\n",
    "\n",
    "    Returns:\n",
    "      a string containing the symbols of regex\n",
    "    \"\"\"\n",
    "\n",
    "    # split regex into single characters\n",
    "    chars = set([*regex])\n",
    "\n",
    "    # remove operators and convert to list\n",
    "    nonoperators = list(chars - set(operators))\n",
    "\n",
    "    # sort them\n",
    "    nonoperators.sort()\n",
    "\n",
    "    # join them into a string\n",
    "    return \"\".join(nonoperators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1133cb0c-40c6-4bac-9889-1555b9bc953f",
   "metadata": {},
   "outputs": [],
   "source": [
    "regexes0 = ['a*', '(a+b)*', 'a']\n",
    "regexes1 = ['a**', '(a*+b*)*', 'b']\n",
    "symbols = ['a', 'ab', 'ab']\n",
    "answer = [True, True, False]\n",
    "\n",
    "for i in range(len(regexes0)):\n",
    "    start0, end0 = regex_to_nfa(regexes0[i])\n",
    "    start1, end1 = regex_to_nfa(regexes1[i])\n",
    "    dfa0 = nfa_to_dfa(start0, end0, regex_to_symbols(regexes0[i]))\n",
    "    dfa1 = nfa_to_dfa(start1, end1, regex_to_symbols(regexes1[i]))\n",
    "    assert are_equivalent_dfas(dfa0, dfa1) == answer[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b0abbe-3947-4d7a-9312-19a47f96c521",
   "metadata": {},
   "source": [
    "Now, finally, we write a function that tests the equivalence of two regular\n",
    "expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f878f685-d8a1-497e-b603-11189f34be5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_equivalent_regexes(regex0, regex1):\n",
    "    \"\"\"\n",
    "    Determine whether two regular expressions recognize the same strings.\n",
    "\n",
    "    Args:\n",
    "      regex0: a regex\n",
    "      regex1: also a regex\n",
    "\n",
    "    Returns:\n",
    "      True iff regex0 and regex1 recognize the same strings.\n",
    "    \"\"\"\n",
    "\n",
    "    start0, end0 = regex_to_nfa(regex0)\n",
    "    start1, end1 = regex_to_nfa(regex1)\n",
    "\n",
    "    symbols0 = regex_to_symbols(regex0)\n",
    "    symbols1 = regex_to_symbols(regex0)\n",
    "    \n",
    "    dfa0 = nfa_to_dfa(start0, end0, symbols0)\n",
    "    dfa1 = nfa_to_dfa(start1, end1, symbols1)\n",
    "\n",
    "    return are_equivalent_dfas(dfa0, dfa1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0490291-7e57-4011-8c44-13f9c5057efc",
   "metadata": {},
   "source": [
    "### Appendix: minimizing DFAs\n",
    "\n",
    "In this section I develop some code to minimize a DFA. I didn't need it for\n",
    "determining the equivalence of regular expressions, but I initially thought I\n",
    "would, and it seemed a shame to delete it.\n",
    "\n",
    "DFA minimization typically requires pruning unreachable states. By\n",
    "construction, the states in the DFAs constructed from NFAs in this notebook are\n",
    "all reachable, so I didn't have to prune them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8de6e4a8-39ac-4397-b1e4-ffdd7a6437ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize(dfa):\n",
    "    equiv = equivalent_states(dfa)\n",
    "\n",
    "    states = {} # map from states of input DFA to states of minimized DFA\n",
    "    \n",
    "    for s in dfa.states:\n",
    "        find = equiv.find(s)\n",
    "\n",
    "        if find not in states:\n",
    "            states[find] = DFAState(find.is_accepting)\n",
    "\n",
    "        states[s] = states[find]\n",
    "\n",
    "    done = set() # states of minimized DFA for which we have next states\n",
    "\n",
    "    for old in dfa.states:\n",
    "        new = states[old]\n",
    "        if new not in done:\n",
    "            new.next_state = {s: states[old.next_state[s]] for s in dfa.symbols}\n",
    "            done.add(new)\n",
    "    \n",
    "    return DFA(done, states[dfa.start], dfa.symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e965bae6-4161-4d29-a371-5f08eae0baf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFA for b*:\n",
      "-----\n",
      "140575041566992\n",
      "accepting\n",
      "\tb:\t140575041566992\n",
      "-----\n",
      "140575041558736\n",
      "start\n",
      "accepting\n",
      "\tb:\t140575041566992\n",
      "Minimized DFA for b*:\n",
      "-----\n",
      "140575041566416\n",
      "start\n",
      "accepting\n",
      "\tb:\t140575041566416\n",
      "DFA for (a+b)*:\n",
      "-----\n",
      "140575041567232\n",
      "accepting\n",
      "\ta:\t140575041567232\n",
      "\tb:\t140575041562480\n",
      "-----\n",
      "140575041559648\n",
      "start\n",
      "accepting\n",
      "\ta:\t140575041567232\n",
      "\tb:\t140575041562480\n",
      "-----\n",
      "140575041562480\n",
      "accepting\n",
      "\ta:\t140575041567232\n",
      "\tb:\t140575041562480\n",
      "Minimized DFA for (a+b)*:\n",
      "-----\n",
      "140575041563296\n",
      "start\n",
      "accepting\n",
      "\ta:\t140575041563296\n",
      "\tb:\t140575041563296\n",
      "DFA for (a*+b*)*:\n",
      "-----\n",
      "140575040982800\n",
      "start\n",
      "accepting\n",
      "\ta:\t140575041567280\n",
      "\tb:\t140575041568288\n",
      "-----\n",
      "140575041568288\n",
      "accepting\n",
      "\ta:\t140575041567280\n",
      "\tb:\t140575041568288\n",
      "-----\n",
      "140575041567280\n",
      "accepting\n",
      "\ta:\t140575041567280\n",
      "\tb:\t140575041568288\n",
      "Minimized DFA for (a*+b*)*:\n",
      "-----\n",
      "140575040989376\n",
      "start\n",
      "accepting\n",
      "\ta:\t140575040989376\n",
      "\tb:\t140575040989376\n"
     ]
    }
   ],
   "source": [
    "regexes = ['b*', '(a+b)*', '(a*+b*)*']\n",
    "symbols = ['b', 'ab', 'ab']\n",
    "\n",
    "for i in range(len(regexes)):\n",
    "    start, end = regex_to_nfa(regexes[i])\n",
    "    dfa = nfa_to_dfa(start, end, symbols[i])\n",
    "    \n",
    "    print(f'DFA for {regexes[i]}:')\n",
    "    print(str(dfa))\n",
    "\n",
    "    min = minimize(dfa)\n",
    "    print(f'Minimized DFA for {regexes[i]}:')\n",
    "    print(str(min))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
